{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jithamanyu001/Correction-Mastero/blob/main/Correction_Mastero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTsQvg_VcQU_",
        "outputId": "eee602ba-77ab-416b-a338-4635b6f59372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda:0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from string import punctuation\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "import torch\n",
        "import gensim\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print ('Device: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "oNofMP4McR17"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "1z2G4VvmckmX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1CUkQSVFY68yrPSvzf7bcu0Rsje854F9A/view?usp=sharing\n",
        "fileDownloaded = drive.CreateFile({'id':'1CUkQSVFY68yrPSvzf7bcu0Rsje854F9A'})"
      ],
      "metadata": {
        "id": "mDFLTwuJfKm3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fileDownloaded.GetContentFile('train.csv')"
      ],
      "metadata": {
        "id": "UKAVjrJehLAu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('train.csv', error_bad_lines=False)\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "YJ2bDF8mhaCK",
        "outputId": "846d336d-a93c-488a-e0be-983a900fce0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78d1c230-efab-44d0-99b7-c34c0a49b518\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78d1c230-efab-44d0-99b7-c34c0a49b518')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78d1c230-efab-44d0-99b7-c34c0a49b518 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78d1c230-efab-44d0-99b7-c34c0a49b518');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "\n",
        "    text = \" \".join(text)\n",
        " # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        lem = WordNetLemmatizer('english')\n",
        "        lem_words = [lem.lemmatize(word) for word in text]\n",
        "        text = \" \".join(lem_words)\n",
        "\n",
        "    # Return a list of words\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "fHfvqUZOiPD_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions_pair = []\n",
        "train_labels = []\n",
        "for _, row in df_train.iterrows():\n",
        "\n",
        "    q1 = text_to_wordlist(str(row['question1']))\n",
        "    q2 = text_to_wordlist(str(row['question2']))\n",
        "    label = int(row['is_duplicate'])\n",
        "    if q1 and q2:\n",
        "        train_questions_pair.append((\n",
        "                q1, q2\n",
        "            ))\n",
        "        train_labels.append(label)\n",
        "\n",
        "print ('Train Data Question Pairs: ', len(train_questions_pair))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Tf5VuWi5zo",
        "outputId": "32d35668-43d8-4e8f-9dc8-b135aea4ad4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Question Pairs:  404270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Language:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words + 1\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words + 1] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "language = Language()\n",
        "for data in [train_questions_pair]:\n",
        "    for question_pair in data:\n",
        "        q1 = question_pair[0]\n",
        "        q2 = question_pair[1]\n",
        "        language.addSentence(q1)\n",
        "        language.addSentence(q2)"
      ],
      "metadata": {
        "id": "KA8Qg3swizon"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet\n",
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "nAuvO3eJkMtI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url='https://www.kaggle.com/datasets/sandreds/googlenewsvectorsnegative300'"
      ],
      "metadata": {
        "id": "DOAq6l6AkNQi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(dataset_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GlQdvoykP65",
        "outputId": "896fc9bb-734c-4cea-f973-4ce527fcb751"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: jithamanyusettur\n",
            "Your Kaggle Key: ··········\n",
            "Downloading googlenewsvectorsnegative300.zip to ./googlenewsvectorsnegative300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.64G/1.64G [01:29<00:00, 19.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_PATH = '/content/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
        "EMBEDDING_DIMENSION = 300\n",
        "EMBEDDING_REQUIRES_GRAD = True\n",
        "HIDDEN_CELLS = 50\n",
        "NUM_LAYERS = 1"
      ],
      "metadata": {
        "id": "LmqdWpWvj9G2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionsDataset(Dataset):\n",
        "    def __init__(self, questions_list, word2index, labels):\n",
        "        self.questions_list = questions_list\n",
        "        self.labels = labels\n",
        "        self.word2index = word2index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        questions_pair = self.questions_list[index]\n",
        "        q1 = questions_pair[0]\n",
        "        q1_indices = []\n",
        "        for word in q1.split():\n",
        "            q1_indices.append(self.word2index[word])\n",
        "\n",
        "        q2 = question_pair[1]\n",
        "        q2_indices = []\n",
        "        for word in q2.split():\n",
        "            q2_indices.append(self.word2index[word])\n",
        "\n",
        "        # q1_indices and q2_indices are lists of indices against words used in the sentence\n",
        "        return q1_indices, q2_indices, self.labels[index]\n",
        "\n",
        "train_dataset = QuestionsDataset(train_questions_pair, language.word2index, train_labels)"
      ],
      "metadata": {
        "id": "aomNIVnHk-0u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocabulary_words = len(language.word2index)\n",
        "print ('Total Unique Vocabulary Words: ', n_vocabulary_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHUXQE3GlEOZ",
        "outputId": "f281306c-40ba-45b3-ab23-ee1b513481f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Unique Vocabulary Words:  86001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCollate:\n",
        "    def custom_collate(self, batch):\n",
        "\n",
        "        # batch = list of tuples where each tuple is of the form ([i1, i2, i3], [j1, j2, j3], label)\n",
        "        q1_list = []\n",
        "        q2_list = []\n",
        "        labels = []\n",
        "        for training_example in batch:\n",
        "          q1_list.append(training_example[0])\n",
        "          q2_list.append(training_example[1])\n",
        "          labels.append(training_example[2])\n",
        "\n",
        "\n",
        "        q1_lengths = [len(q) for q in q1_list]\n",
        "        q2_lengths = [len(q) for q in q2_list]\n",
        "\n",
        "        return q1_list, q1_lengths, q2_list, q2_lengths, labels\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.custom_collate(batch)"
      ],
      "metadata": {
        "id": "Le0IItnFlGmc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_split = 0.2\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "shuffle_dataset = True\n",
        "random_seed = 32\n",
        "\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=500, sampler=train_sampler, collate_fn=CustomCollate())\n",
        "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=500, sampler=validation_sampler, collate_fn=CustomCollate())\n",
        "\n",
        "print ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZQtYoaslJGJ",
        "outputId": "5bd9ab21-d2de-4428-e0bd-973d7c0824ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Size 323416, Validation Set Size 80854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_PATH, binary=True)\n",
        "# Convert word2vec embeddings into FloatTensor\n",
        "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
        "\n",
        "# Create a random weight tensor of the shape (n_vocabulary_words + 1, EMBEDDING_DIMENSION) and place each word's embedding from word2vec at the index assigned to that word\n",
        "# Two key points:\n",
        "# 1. Weights tensor has been initialized randomly so that the words which are part of our dataset vocabulary but are not present in word2vec are given a random embedding\n",
        "# 2. Embedding at 0 index is all zeros. This is the embedding for the padding that we will do for batch processing\n",
        "weights = torch.randn(n_vocabulary_words + 1, EMBEDDING_DIMENSION)\n",
        "weights[0] = torch.zeros(EMBEDDING_DIMENSION)\n",
        "for word, lang_word_index in language.word2index.items():\n",
        "    if word in word2vec_model:\n",
        "        weights[lang_word_index] = torch.FloatTensor(word2vec_model.word_vec(word))\n",
        "\n",
        "del word2vec_model\n",
        "del word2vec_weights"
      ],
      "metadata": {
        "id": "V32nFExklMkt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, pretrained_weights=None,vocabLen=None):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        if(vocabLen==None):\n",
        "          self.embedding = nn.Embedding.from_pretrained(pretrained_weights)\n",
        "          self.embedding.weight.requires_grad = EMBEDDING_REQUIRES_GRAD\n",
        "        else:\n",
        "          self.embedding=nn.Embedding(vocabLen,300)\n",
        "        # Create a single LSTM since this is a Siamese Network and the weights are shared\n",
        "        self.lstm = nn.LSTM(input_size=EMBEDDING_DIMENSION, hidden_size=HIDDEN_CELLS, num_layers = NUM_LAYERS, batch_first = True)\n",
        "\n",
        "    # Manhattan Distance Calculator\n",
        "    def exponent_neg_manhattan_distance(self, x1, x2):\n",
        "        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=0)).to(device)\n",
        "\n",
        "    def forward_once(self, x, input_lengths):\n",
        "\n",
        "        # x is of the shape (batch_dim, sequence)\n",
        "        # e.g. x = [\n",
        "        #  [i1, i2, i3],\n",
        "        #  [j1, j2, j3, j4]\n",
        "        # ]\n",
        "\n",
        "        # input_lengths is the list that contains the sequence lengths for each sequence\n",
        "        # e.g. input_lengths = [3, 4]\n",
        "\n",
        "        # Reverse sequence lengths indices in decreasing order as per the requirement from PyTorch before Padding and Packing\n",
        "        sorted_indices = np.flipud(np.argsort(input_lengths))\n",
        "        input_lengths = np.flipud(np.sort(input_lengths))\n",
        "        input_lengths = input_lengths.copy() # https://github.com/facebookresearch/InferSent/issues/99\n",
        "\n",
        "        # Reorder questions in the decreasing order of their lengths\n",
        "        ordered_questions = [torch.LongTensor(x[i]).to(device) for i in sorted_indices]\n",
        "        # Pad sequences with 0s to the max length sequence in the batch\n",
        "        ordered_questions = torch.nn.utils.rnn.pad_sequence(ordered_questions, batch_first=True)\n",
        "        # Retrieve Embeddings\n",
        "        embeddings = self.embedding(ordered_questions).to(device)\n",
        "        # Pack the padded sequences and pass it through LSTM\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, input_lengths, batch_first=True)\n",
        "        out, (hn, cn) = self.lstm(packed)\n",
        "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=int(input_lengths[0]))\n",
        "        # The following step reorders the calculated activations to the original order in which questions were passed\n",
        "        result = torch.FloatTensor(unpacked.size())\n",
        "        for i, encoded_matrix in enumerate(unpacked):\n",
        "            result[sorted_indices[i]] = encoded_matrix\n",
        "        return result\n",
        "\n",
        "    def forward(self, q1, q1_lengths, q2, q2_lengths):\n",
        "        output1 = self.forward_once(q1, q1_lengths)\n",
        "        output2 = self.forward_once(q2, q2_lengths)\n",
        "        similarity_score = torch.zeros(output1.size()[0]).to(device)\n",
        "        # Calculate Similarity Score between both questions in a single pair\n",
        "        for index in range(output1.size()[0]):\n",
        "            # Sequence lenghts are being used to index and retrieve the activations before the zero padding since they were not part of original question\n",
        "            q1 = output1[index, q1_lengths[index] - 1, :]\n",
        "            q2 = output2[index, q2_lengths[index] - 1, :]\n",
        "            similarity_score[index] = self.exponent_neg_manhattan_distance(q1, q2)\n",
        "        return similarity_score\n",
        "\n",
        "model = SiameseNetwork(vocabLen=n_vocabulary_words+1).to(device)"
      ],
      "metadata": {
        "id": "vmfJk3d2lfCk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(customLoss,self).__init__()\n",
        "  def forward(self,y,y_hat):\n",
        "    return torch.log(1+0.01*(y-y_hat)**2)"
      ],
      "metadata": {
        "id": "iutYPcMlb3aD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "QSd6NBV9lsxU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(train_loader)\n",
        "# Threshold 0.5. Since similarity score will be a value between 0 and 1, we will consider all question pair with values greater than threshold as Duplicate\n",
        "threshold = torch.Tensor([0.5]).to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_history = []\n",
        "    model.train(True)\n",
        "    train_correct_total = 0\n",
        "    for i, (q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths, labels) in enumerate(train_loader):\n",
        "\n",
        "        labels = torch.FloatTensor(labels).to(device)\n",
        "\n",
        "        # Clear grads\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run the forward pass\n",
        "        similarity_score = model(q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths)\n",
        "        predictions = (similarity_score > threshold).float() * 1\n",
        "        total = labels.size()[0]\n",
        "        correct = (predictions == labels).sum().item()\n",
        "        train_correct_total += correct\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = criterion(similarity_score, labels)\n",
        "\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            loss_history.append(loss.item())\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, np.mean(loss_history), (correct / total) * 100))\n",
        "\n",
        "    print('Training Loss: {:.4f}, Training Accuracy: {:.4f}'.format(np.mean(loss_history), (train_correct_total / len(train_indices)) * 100))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI0kUDBTme6U",
        "outputId": "550296c5-451e-4ef4-c162-2510d2812210"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/647], Loss: 1.1192, Accuracy: 65.4000\n",
            "Epoch [1/10], Step [200/647], Loss: 0.9581, Accuracy: 66.4000\n",
            "Epoch [1/10], Step [300/647], Loss: 0.9035, Accuracy: 62.0000\n",
            "Epoch [1/10], Step [400/647], Loss: 0.8457, Accuracy: 70.8000\n",
            "Epoch [1/10], Step [500/647], Loss: 0.7974, Accuracy: 69.4000\n",
            "Epoch [1/10], Step [600/647], Loss: 0.7653, Accuracy: 69.6000\n",
            "Training Loss: 0.7653, Training Accuracy: 66.2985\n",
            "Epoch [2/10], Step [100/647], Loss: 0.5968, Accuracy: 69.2000\n",
            "Epoch [2/10], Step [200/647], Loss: 0.5856, Accuracy: 70.4000\n",
            "Epoch [2/10], Step [300/647], Loss: 0.5836, Accuracy: 67.0000\n",
            "Epoch [2/10], Step [400/647], Loss: 0.5723, Accuracy: 71.6000\n",
            "Epoch [2/10], Step [500/647], Loss: 0.5762, Accuracy: 67.8000\n",
            "Epoch [2/10], Step [600/647], Loss: 0.5757, Accuracy: 70.6000\n",
            "Training Loss: 0.5757, Training Accuracy: 70.1474\n",
            "Epoch [3/10], Step [100/647], Loss: 0.5718, Accuracy: 68.8000\n",
            "Epoch [3/10], Step [200/647], Loss: 0.5432, Accuracy: 74.8000\n",
            "Epoch [3/10], Step [300/647], Loss: 0.5492, Accuracy: 71.0000\n",
            "Epoch [3/10], Step [400/647], Loss: 0.5534, Accuracy: 72.0000\n",
            "Epoch [3/10], Step [500/647], Loss: 0.5496, Accuracy: 75.0000\n",
            "Epoch [3/10], Step [600/647], Loss: 0.5494, Accuracy: 71.2000\n",
            "Training Loss: 0.5494, Training Accuracy: 72.5957\n",
            "Epoch [4/10], Step [100/647], Loss: 0.5291, Accuracy: 72.4000\n",
            "Epoch [4/10], Step [200/647], Loss: 0.5195, Accuracy: 75.0000\n",
            "Epoch [4/10], Step [300/647], Loss: 0.5107, Accuracy: 73.8000\n",
            "Epoch [4/10], Step [400/647], Loss: 0.5199, Accuracy: 70.8000\n",
            "Epoch [4/10], Step [500/647], Loss: 0.5147, Accuracy: 76.4000\n",
            "Epoch [4/10], Step [600/647], Loss: 0.5175, Accuracy: 72.8000\n",
            "Training Loss: 0.5175, Training Accuracy: 74.8726\n",
            "Epoch [5/10], Step [100/647], Loss: 0.4711, Accuracy: 77.8000\n",
            "Epoch [5/10], Step [200/647], Loss: 0.4563, Accuracy: 81.0000\n",
            "Epoch [5/10], Step [300/647], Loss: 0.4734, Accuracy: 72.4000\n",
            "Epoch [5/10], Step [400/647], Loss: 0.4773, Accuracy: 75.8000\n",
            "Epoch [5/10], Step [500/647], Loss: 0.4870, Accuracy: 75.2000\n",
            "Epoch [5/10], Step [600/647], Loss: 0.4903, Accuracy: 74.0000\n",
            "Training Loss: 0.4903, Training Accuracy: 77.1322\n",
            "Epoch [6/10], Step [100/647], Loss: 0.4465, Accuracy: 79.6000\n",
            "Epoch [6/10], Step [200/647], Loss: 0.4459, Accuracy: 78.8000\n",
            "Epoch [6/10], Step [300/647], Loss: 0.4459, Accuracy: 78.6000\n",
            "Epoch [6/10], Step [400/647], Loss: 0.4373, Accuracy: 80.8000\n",
            "Epoch [6/10], Step [500/647], Loss: 0.4359, Accuracy: 80.4000\n",
            "Epoch [6/10], Step [600/647], Loss: 0.4366, Accuracy: 79.8000\n",
            "Training Loss: 0.4366, Training Accuracy: 79.0873\n",
            "Epoch [7/10], Step [100/647], Loss: 0.3660, Accuracy: 83.6000\n",
            "Epoch [7/10], Step [200/647], Loss: 0.3986, Accuracy: 80.4000\n",
            "Epoch [7/10], Step [300/647], Loss: 0.3996, Accuracy: 80.4000\n",
            "Epoch [7/10], Step [400/647], Loss: 0.4022, Accuracy: 81.4000\n",
            "Epoch [7/10], Step [500/647], Loss: 0.4017, Accuracy: 82.6000\n",
            "Epoch [7/10], Step [600/647], Loss: 0.4040, Accuracy: 81.4000\n",
            "Training Loss: 0.4040, Training Accuracy: 80.7267\n",
            "Epoch [8/10], Step [100/647], Loss: 0.3623, Accuracy: 86.0000\n",
            "Epoch [8/10], Step [200/647], Loss: 0.3716, Accuracy: 82.0000\n",
            "Epoch [8/10], Step [300/647], Loss: 0.3644, Accuracy: 86.2000\n",
            "Epoch [8/10], Step [400/647], Loss: 0.3822, Accuracy: 78.2000\n",
            "Epoch [8/10], Step [500/647], Loss: 0.3846, Accuracy: 81.8000\n",
            "Epoch [8/10], Step [600/647], Loss: 0.3856, Accuracy: 80.4000\n",
            "Training Loss: 0.3856, Training Accuracy: 82.1236\n",
            "Epoch [9/10], Step [100/647], Loss: 0.3711, Accuracy: 83.0000\n",
            "Epoch [9/10], Step [200/647], Loss: 0.3542, Accuracy: 85.0000\n",
            "Epoch [9/10], Step [300/647], Loss: 0.3575, Accuracy: 82.4000\n",
            "Epoch [9/10], Step [400/647], Loss: 0.3596, Accuracy: 84.0000\n",
            "Epoch [9/10], Step [500/647], Loss: 0.3625, Accuracy: 82.4000\n",
            "Epoch [9/10], Step [600/647], Loss: 0.3627, Accuracy: 84.4000\n",
            "Training Loss: 0.3627, Training Accuracy: 83.4121\n",
            "Epoch [10/10], Step [100/647], Loss: 0.3196, Accuracy: 87.8000\n",
            "Epoch [10/10], Step [200/647], Loss: 0.3217, Accuracy: 85.2000\n",
            "Epoch [10/10], Step [300/647], Loss: 0.3307, Accuracy: 83.6000\n",
            "Epoch [10/10], Step [400/647], Loss: 0.3296, Accuracy: 85.8000\n",
            "Epoch [10/10], Step [500/647], Loss: 0.3285, Accuracy: 85.8000\n",
            "Epoch [10/10], Step [600/647], Loss: 0.3268, Accuracy: 88.0000\n",
            "Training Loss: 0.3268, Training Accuracy: 84.4232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model.train(False)\n",
        "    val_correct_total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths, labels) in enumerate(val_loader):\n",
        "\n",
        "            labels = torch.FloatTensor(labels).to(device)\n",
        "\n",
        "            similarity_score = model(q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths)\n",
        "            predictions = (similarity_score > threshold).float() * 1\n",
        "            total = labels.size()[0]\n",
        "            correct = (predictions == labels).sum().item()\n",
        "            val_correct_total += correct\n",
        "\n",
        "        avg_acc_val =  val_correct_total * 100 / len(val_indices)\n",
        "        print ('Validation Set Size {}, Correct in Validation {}, Validation Accuracy {:.2f}%'.format(len(val_indices), val_correct_total, avg_acc_val))"
      ],
      "metadata": {
        "id": "i7_XisNcpY50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b109c5c6-520f-4e4e-d716-9951cdc665a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Set Size 80854, Correct in Validation 61382, Validation Accuracy 75.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyVbuudVsE51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}